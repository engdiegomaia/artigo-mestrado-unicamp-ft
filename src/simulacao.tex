% Capitulo de Implementacao e Simulacao
\chapter{Implementacao e Simulacao}\label{chp:simulacao}

Este capitulo apresenta a implementacao pratica das estrategias de otimizacao em hardware embarcado, incluindo o desenvolvimento de designs VHDL para FPGAs, simulacoes detalhadas via GHDL, implementacoes para VPUs e GPUs embarcadas, e a criacao de prototipos funcionais. O foco esta na validacao tecnica das estrategias propostas e na demonstracao de viabilidade em sistemas reais.

\section{Implementacao em FPGA via GHDL}\label{sec:implementacao_fpga}

A implementacao em FPGA constitui a principal contribuicao tecnica deste trabalho, demonstrando como algoritmos hiperespectrais podem ser otimizados para hardware reconfiguravel com eficiencia energetica superior.

\subsection{Arquitetura do Sistema}

\subsubsection{Visao Geral da Arquitetura}
O sistema FPGA foi projetado como uma arquitetura pipeline multi-estagio que maximiza throughput enquanto minimiza latencia e consumo energetico:

\begin{figure}[!htb]
\centering
% Figura sera incluida posteriormente
% \includegraphics[width=0.9\textwidth]{fpga_architecture.png}
\caption[Arquitetura FPGA]{Arquitetura do sistema FPGA para processamento hiperespectral embarcado.}
\label{fig:fpga_architecture}
\end{figure}

Os principais modulos da arquitetura incluem:
\begin{itemize}
    \item \textbf{Input Interface}: Recepcao e buffering de dados hiperespectrais
    \item \textbf{Preprocessing Unit}: Correcao radiometrica e normalizacao
    \item \textbf{Compression Engine}: Compressao adaptativa espectral
    \item \textbf{PCA Engine}: Reducao de dimensionalidade incremental
    \item \textbf{Classification Core}: Classificacao otimizada
    \item \textbf{Output Controller}: Formatacao e comunicacao de resultados
\end{itemize}

\subsubsection{Estrategia de Pipeline}
O pipeline foi projetado para operar com 5 estagios principais, permitindo processamento simultaneo de multiplos pixels:

\begin{lstlisting}[language=VHDL]
-- Pipeline de Processamento Principal
entity hyperspectral_pipeline is
    generic (
        BANDS         : integer := 224;
        PIXEL_WIDTH   : integer := 16;
        PIPELINE_DEPTH: integer := 5;
        PARALLEL_UNITS: integer := 8
    );
    port (
        clk           : in std_logic;
        rst           : in std_logic;
        -- Interface de entrada
        pixel_data    : in std_logic_vector(PIXEL_WIDTH-1 downto 0);
        pixel_valid   : in std_logic;
        band_index    : in std_logic_vector(7 downto 0);
        -- Interface de saida
        result_data   : out std_logic_vector(PIXEL_WIDTH-1 downto 0);
        result_valid  : out std_logic;
        result_class  : out std_logic_vector(3 downto 0)
    );
end entity;
\end{lstlisting}

\subsection{Modulos Especializados}

\subsubsection{Engine de Compressao Adaptativa}
O modulo de compressao implementa a selecao adaptativa de bandas baseada em correlacao:

\begin{lstlisting}[language=VHDL]
entity adaptive_compression is
    generic (
        BANDS : integer := 224;
        THRESHOLD : integer := 61440  -- 0.95 * 2^16
    );
    port (
        clk : in std_logic;
        rst : in std_logic;
        -- Entrada de dados espectrais
        spectral_data : in spectral_array_t;
        data_valid : in std_logic;
        -- Saida comprimida
        compressed_data : out compressed_array_t;
        compression_ratio : out std_logic_vector(7 downto 0);
        output_valid : out std_logic
    );
end entity;

architecture behavioral of adaptive_compression is
    -- Sinais internos para correlacao
    signal correlation_matrix : correlation_matrix_t;
    signal selected_bands : band_selection_t;
    signal correlation_engine : correlation_unit_t;
    
begin
    -- Processo de calculo de correlacao em tempo real
    correlation_proc: process(clk)
    begin
        if rising_edge(clk) then
            if rst = '1' then
                correlation_matrix <= (others => (others => 0));
                selected_bands <= (others => '0');
            elsif data_valid = '1' then
                -- Atualizar matriz de correlacao incrementalmente
                update_correlation_matrix(spectral_data, correlation_matrix);
                -- Selecionar bandas com baixa correlacao
                select_bands(correlation_matrix, THRESHOLD, selected_bands);
            end if;
        end if;
    end process;
    
    -- Compressao baseada na selecao de bandas
    compression_proc: process(clk)
    begin
        if rising_edge(clk) then
            for i in 0 to BANDS-1 loop
                if selected_bands(i) = '1' then
                    compressed_data(get_compressed_index(i)) <= spectral_data(i);
                end if;
            end loop;
            
            compression_ratio <= calculate_ratio(selected_bands);
            output_valid <= data_valid;
        end if;
    end process;
end architecture;
\end{lstlisting}

\subsubsection{Engine PCA Incremental}
Implementacao do PCA incremental otimizada para streaming de dados:

\begin{lstlisting}[language=VHDL]
entity incremental_pca is
    generic (
        INPUT_DIM : integer := 224;
        OUTPUT_DIM : integer := 20;
        DATA_WIDTH : integer := 16
    );
    port (
        clk : in std_logic;
        rst : in std_logic;
        -- Entrada de dados
        input_vector : in std_logic_vector(INPUT_DIM*DATA_WIDTH-1 downto 0);
        input_valid : in std_logic;
        -- Saida transformada
        output_vector : out std_logic_vector(OUTPUT_DIM*DATA_WIDTH-1 downto 0);
        output_valid : out std_logic;
        -- Status de convergencia
        convergence_flag : out std_logic
    );
end entity;

architecture behavioral of incremental_pca is
    -- Componentes principais armazenados
    signal principal_components : component_matrix_t;
    signal mean_vector : mean_vector_t;
    signal sample_count : integer := 0;
    
    -- Unidades de processamento paralelo
    component matrix_multiply is
        generic (
            ROWS : integer;
            COLS : integer;
            DATA_WIDTH : integer
        );
        port (
            clk : in std_logic;
            matrix_a : in matrix_t;
            vector_b : in vector_t;
            result : out vector_t;
            valid : out std_logic
        );
    end component;
    
begin
    -- Atualizacao incremental da media
    mean_update_proc: process(clk)
    variable new_mean : mean_vector_t;
    begin
        if rising_edge(clk) then
            if rst = '1' then
                mean_vector <= (others => 0);
                sample_count <= 0;
            elsif input_valid = '1' then
                sample_count <= sample_count + 1;
                
                -- Atualizacao incremental: mean = (old_mean * (n-1) + new_sample) / n
                for i in 0 to INPUT_DIM-1 loop
                    new_mean(i) := (mean_vector(i) * (sample_count - 1) + 
                                   get_input_component(input_vector, i)) / sample_count;
                end loop;
                
                mean_vector <= new_mean;
            end if;
        end if;
    end process;
    
    -- Transformacao PCA usando componentes atuais
    pca_transform: matrix_multiply
        generic map (
            ROWS => OUTPUT_DIM,
            COLS => INPUT_DIM,
            DATA_WIDTH => DATA_WIDTH
        )
        port map (
            clk => clk,
            matrix_a => principal_components,
            vector_b => subtract_mean(input_vector, mean_vector),
            result => output_vector,
            valid => output_valid
        );
        
    -- Verificacao de convergencia baseada em variacao dos componentes
    convergence_check: process(clk)
    variable component_change : std_logic_vector(15 downto 0);
    begin
        if rising_edge(clk) then
            component_change := calculate_component_variation(principal_components);
            
            if component_change < CONVERGENCE_THRESHOLD then
                convergence_flag <= '1';
            else
                convergence_flag <= '0';
            end if;
        end if;
    end process;
end architecture;
\end{lstlisting}

\subsubsection{Unidade de Classificacao Otimizada}
Implementacao de SVM otimizada para hardware com precisao adaptativa:

\begin{lstlisting}[language=VHDL]
entity optimized_classifier is
    generic (
        FEATURE_DIM : integer := 20;
        NUM_CLASSES : integer := 16;
        DATA_WIDTH : integer := 16
    );
    port (
        clk : in std_logic;
        rst : in std_logic;
        -- Entrada de caracteristicas
        features : in std_logic_vector(FEATURE_DIM*DATA_WIDTH-1 downto 0);
        features_valid : in std_logic;
        -- Saida de classificacao
        class_result : out std_logic_vector(3 downto 0);
        confidence : out std_logic_vector(7 downto 0);
        result_valid : out std_logic
    );
end entity;

architecture behavioral of optimized_classifier is
    -- Modelos pre-treinados armazenados em BRAM
    signal support_vectors : support_vector_matrix_t;
    signal weights : weight_vector_t;
    signal bias : bias_vector_t;
    
    -- Unidades de calculo paralelo
    type decision_array_t is array (0 to NUM_CLASSES-1) of signed(DATA_WIDTH-1 downto 0);
    signal decision_values : decision_array_t;
    
begin
    -- Calculo paralelo de decisoes para todas as classes
    class_decision_gen: for i in 0 to NUM_CLASSES-1 generate
        svm_kernel: entity work.rbf_kernel
            generic map (
                FEATURE_DIM => FEATURE_DIM,
                DATA_WIDTH => DATA_WIDTH
            )
            port map (
                clk => clk,
                input_vector => features,
                support_vectors => support_vectors(i),
                weights => weights(i),
                bias => bias(i),
                decision_value => decision_values(i)
            );
    end generate;
    
    -- Selecao da classe com maior valor de decisao
    winner_selection: process(clk)
    variable max_decision : signed(DATA_WIDTH-1 downto 0);
    variable winner_class : integer range 0 to NUM_CLASSES-1;
    begin
        if rising_edge(clk) then
            if features_valid = '1' then
                max_decision := decision_values(0);
                winner_class := 0;
                
                for i in 1 to NUM_CLASSES-1 loop
                    if decision_values(i) > max_decision then
                        max_decision := decision_values(i);
                        winner_class := i;
                    end if;
                end loop;
                
                class_result <= std_logic_vector(to_unsigned(winner_class, 4));
                confidence <= calculate_confidence(max_decision, decision_values);
                result_valid <= '1';
            else
                result_valid <= '0';
            end if;
        end if;
    end process;
end architecture;
\end{lstlisting}

\subsection{Simulacao e Validacao via GHDL}

\subsubsection{Ambiente de Simulacao}
O ambiente de simulacao foi configurado para validar tanto correcao funcional quanto performance temporal:

\begin{lstlisting}[language=VHDL]
-- Testbench principal para validacao do sistema completo
entity hyperspectral_tb is
end entity;

architecture sim of hyperspectral_tb is
    -- Sinais de clock e reset
    signal clk : std_logic := '0';
    signal rst : std_logic := '1';
    
    -- Sinais do DUT (Device Under Test)
    signal pixel_data : std_logic_vector(15 downto 0);
    signal pixel_valid : std_logic := '0';
    signal result_data : std_logic_vector(15 downto 0);
    signal result_valid : std_logic;
    
    -- Controle de simulacao
    constant CLK_PERIOD : time := 10 ns;  -- 100 MHz
    signal sim_finished : boolean := false;
    
    -- Metricas de performance
    signal latency_counter : integer := 0;
    signal throughput_counter : integer := 0;
    
begin
    -- Geracao de clock
    clk_gen: clk <= not clk after CLK_PERIOD/2 when not sim_finished;
    
    -- Instanciacao do DUT
    dut: entity work.hyperspectral_pipeline
        generic map (
            BANDS => 224,
            PIXEL_WIDTH => 16,
            PIPELINE_DEPTH => 5,
            PARALLEL_UNITS => 8
        )
        port map (
            clk => clk,
            rst => rst,
            pixel_data => pixel_data,
            pixel_valid => pixel_valid,
            result_data => result_data,
            result_valid => result_valid
        );
    
    -- Processo de estimulo baseado em dataset real
    stimulus_proc: process
        file input_file : text open read_mode is "indian_pines_adapted.txt";
        variable line_buffer : line;
        variable pixel_value : integer;
    begin
        -- Reset inicial
        rst <= '1';
        wait for 100 ns;
        rst <= '0';
        wait for 50 ns;
        
        -- Carregamento e processamento de dados de teste
        while not endfile(input_file) loop
            readline(input_file, line_buffer);
            read(line_buffer, pixel_value);
            
            pixel_data <= std_logic_vector(to_unsigned(pixel_value, 16));
            pixel_valid <= '1';
            wait for CLK_PERIOD;
            pixel_valid <= '0';
            
            -- Simulacao de intervalo entre pixels
            wait for CLK_PERIOD * 3;
        end loop;
        
        -- Aguardar processamento final
        wait for CLK_PERIOD * 1000;
        sim_finished <= true;
        wait;
    end process;
    
    -- Monitoramento de metricas
    metrics_proc: process(clk)
    begin
        if rising_edge(clk) then
            -- Contagem de latencia
            if pixel_valid = '1' then
                latency_counter <= 0;
            elsif latency_counter < 1000 then
                latency_counter <= latency_counter + 1;
            end if;
            
            -- Contagem de throughput
            if result_valid = '1' then
                throughput_counter <= throughput_counter + 1;
                
                -- Report de latencia quando resultado e valido
                report "Latency: " & integer'image(latency_counter) & " cycles";
            end if;
        end if;
    end process;
    
    -- Validacao de resultados
    result_check: process(clk)
        file output_file : text open write_mode is "simulation_results.txt";
        variable outline : line;
    begin
        if rising_edge(clk) and result_valid = '1' then
            -- Escrever resultados para arquivo
            write(outline, to_integer(unsigned(result_data)));
            writeline(output_file, outline);
            
            -- Verificacao de sanidade dos resultados
            assert to_integer(unsigned(result_data)) >= 0 and 
                   to_integer(unsigned(result_data)) <= 15
                report "Invalid classification result!" severity error;
        end if;
    end process;
end architecture;
\end{lstlisting}

\subsubsection{Analise de Timing}
Analise detalhada dos caminhos criticos e otimizacao temporal:

\begin{lstlisting}[language=sh]
# Script de simulacao e analise via GHDL
#!/bin/bash

# Compilacao dos modulos VHDL
ghdl -a --std=08 --work=work packages.vhd
ghdl -a --std=08 --work=work adaptive_compression.vhd
ghdl -a --std=08 --work=work incremental_pca.vhd
ghdl -a --std=08 --work=work optimized_classifier.vhd
ghdl -a --std=08 --work=work hyperspectral_pipeline.vhd
ghdl -a --std=08 --work=work hyperspectral_tb.vhd

# Elaboracao do testbench
ghdl -e --std=08 --work=work hyperspectral_tb

# Simulacao com geracao de VCD para analise
ghdl -r hyperspectral_tb --vcd=simulation.vcd --stop-time=10ms

# Analise de timing usando GTKWave
gtkwave simulation.vcd &

# Extracao de metricas de performance
echo "=== Performance Metrics ==="
grep "Latency:" simulation.log | awk '{sum+=$2; count++} END {print "Average Latency:", sum/count, "cycles"}'
grep "Throughput" simulation.log | tail -1 | awk '{print "Total Throughput:", $2, "pixels processed"}'

# Analise de utilizacao de recursos
echo "=== Resource Utilization ==="
ghdl -a --std=08 --syn-binding --work=work hyperspectral_pipeline.vhd 2>&1 | grep -E "(LUT|FF|DSP|BRAM)"
\end{lstlisting}

\subsubsection{Analise de Consumo Energetico}
Estimativa de consumo baseada em modelos de switching activity:

\begin{lstlisting}[language=VHDL]
-- Package para calculo de consumo energetico
package power_estimation is
    -- Constantes de consumo por tipo de recurso (em mW)
    constant LUT_POWER : real := 0.05;
    constant FF_POWER : real := 0.02;
    constant DSP_POWER : real := 2.5;
    constant BRAM_POWER : real := 1.2;
    
    -- Funcao para calculo de atividade de switching
    function calculate_switching_activity(
        signal_vector : std_logic_vector;
        prev_vector : std_logic_vector
    ) return real;
    
    -- Funcao para estimativa de consumo total
    function estimate_power_consumption(
        lut_count : integer;
        ff_count : integer;
        dsp_count : integer;
        bram_count : integer;
        switching_factor : real
    ) return real;
end package;

package body power_estimation is
    function calculate_switching_activity(
        signal_vector : std_logic_vector;
        prev_vector : std_logic_vector
    ) return real is
        variable transitions : integer := 0;
    begin
        for i in signal_vector'range loop
            if signal_vector(i) /= prev_vector(i) then
                transitions := transitions + 1;
            end if;
        end loop;
        
        return real(transitions) / real(signal_vector'length);
    end function;
    
    function estimate_power_consumption(
        lut_count : integer;
        ff_count : integer;
        dsp_count : integer;
        bram_count : integer;
        switching_factor : real
    ) return real is
    begin
        return (real(lut_count) * LUT_POWER + 
                real(ff_count) * FF_POWER +
                real(dsp_count) * DSP_POWER +
                real(bram_count) * BRAM_POWER) * switching_factor;
    end function;
end package body;
\end{lstlisting}

\section{Implementacao em VPU}\label{sec:implementacao_vpu}

A implementacao para Vision Processing Units utilizou o Intel OpenVINO toolkit para otimizacao de algoritmos hiperespectrais.

\subsection{Arquitetura de Software}

\subsubsection{Pipeline de Processamento}
Implementacao otimizada para a arquitetura Myriad X:

\begin{lstlisting}[language=Python]
import openvino.runtime as ov
import numpy as np
from typing import List, Tuple
import cv2

class HyperspectralVPU:
    def __init__(self, model_path: str, device: str = "MYRIAD"):
        self.core = ov.Core()
        self.model = self.core.read_model(model=model_path)
        self.compiled_model = self.core.compile_model(
            model=self.model, 
            device_name=device
        )
        
        # Configuracoes otimizadas para VPU
        self.input_layer = self.compiled_model.input(0)
        self.output_layer = self.compiled_model.output(0)
        
        # Cache para otimizacao
        self.pca_components = None
        self.selected_bands = None
        
    def preprocess_hyperspectral(self, 
                               hypercube: np.ndarray) -> np.ndarray:
        """
        Pre-processamento otimizado para VPU
        """
        # Normalizacao eficiente
        normalized = self._fast_normalize(hypercube)
        
        # Selecao adaptativa de bandas
        if self.selected_bands is None:
            self.selected_bands = self._select_bands_correlation(normalized)
        
        # Aplicar selecao de bandas
        compressed = normalized[:, :, self.selected_bands]
        
        # Reducao de dimensionalidade via PCA incremental
        if self.pca_components is None:
            self.pca_components = self._initialize_pca(compressed)
        
        pca_data = self._apply_pca_incremental(compressed)
        
        return pca_data
    
    def _fast_normalize(self, data: np.ndarray) -> np.ndarray:
        """
        Normalizacao otimizada usando operacoes vectoriais
        """
        # Normalizacao por banda para reduzir variacao espectral
        mean_per_band = np.mean(data, axis=(0, 1), keepdims=True)
        std_per_band = np.std(data, axis=(0, 1), keepdims=True)
        
        # Evitar divisao por zero
        std_per_band = np.where(std_per_band == 0, 1, std_per_band)
        
        normalized = (data - mean_per_band) / std_per_band
        
        # Clipping para evitar outliers
        return np.clip(normalized, -3, 3)
    
    def _select_bands_correlation(self, 
                                data: np.ndarray, 
                                threshold: float = 0.95) -> List[int]:
        """
        Selecao de bandas baseada em correlacao
        """
        # Reshape para analise de correlacao
        reshaped = data.reshape(-1, data.shape[2])
        correlation_matrix = np.corrcoef(reshaped.T)
        
        selected_bands = [0]  # Sempre incluir primeira banda
        
        for i in range(1, correlation_matrix.shape[0]):
            max_corr = max([abs(correlation_matrix[i, j]) 
                           for j in selected_bands])
            
            if max_corr < threshold:
                selected_bands.append(i)
        
        print(f"Selected {len(selected_bands)} bands from {data.shape[2]}")
        return selected_bands
    
    def _initialize_pca(self, data: np.ndarray, 
                       n_components: int = 20) -> np.ndarray:
        """
        Inicializacao do PCA para processamento incremental
        """
        # Reshape para PCA
        reshaped = data.reshape(-1, data.shape[2])
        
        # PCA usando SVD truncada para eficiencia
        U, s, Vt = np.linalg.svd(reshaped.T @ reshaped, full_matrices=False)
        
        # Selecionar componentes principais mais significativos
        components = Vt[:n_components, :]
        
        return components
    
    def _apply_pca_incremental(self, data: np.ndarray) -> np.ndarray:
        """
        Aplicacao incremental do PCA
        """
        original_shape = data.shape[:2]
        reshaped = data.reshape(-1, data.shape[2])
        
        # Transformacao PCA
        transformed = reshaped @ self.pca_components.T
        
        # Reshape de volta para formato de imagem
        return transformed.reshape(*original_shape, -1)
    
    def process_tile(self, tile: np.ndarray) -> Tuple[int, float]:
        """
        Processamento de um tile da imagem hiperespectral
        """
        # Pre-processamento
        processed_tile = self.preprocess_hyperspectral(tile)
        
        # Preparar entrada para VPU
        input_data = self._prepare_vpu_input(processed_tile)
        
        # Inferencia na VPU
        request = self.compiled_model.create_infer_request()
        request.infer({self.input_layer: input_data})
        
        # Extrair resultados
        output = request.get_output_tensor().data
        
        # Pos-processamento
        class_id, confidence = self._postprocess_output(output)
        
        return class_id, confidence
    
    def _prepare_vpu_input(self, data: np.ndarray) -> np.ndarray:
        """
        Preparacao especifica para entrada da VPU
        """
        # Normalizacao para range esperado pelo modelo
        normalized = (data - data.min()) / (data.max() - data.min())
        
        # Conversao para formato esperado (NCHW)
        if len(data.shape) == 3:
            # Adicionar dimensao de batch
            data = np.expand_dims(data, axis=0)
            # Transpor para formato NCHW
            data = np.transpose(data, (0, 3, 1, 2))
        
        return data.astype(np.float32)
    
    def _postprocess_output(self, output: np.ndarray) -> Tuple[int, float]:
        """
        Pos-processamento da saida da VPU
        """
        # Aplicar softmax para obter probabilidades
        probabilities = self._softmax(output[0])
        
        # Encontrar classe com maior probabilidade
        class_id = np.argmax(probabilities)
        confidence = probabilities[class_id]
        
        return int(class_id), float(confidence)
    
    def _softmax(self, x: np.ndarray) -> np.ndarray:
        """
        Funcao softmax otimizada
        """
        exp_x = np.exp(x - np.max(x))  # Estabilidade numerica
        return exp_x / np.sum(exp_x)
    
    def process_streaming(self, 
                         data_stream: iter, 
                         callback: callable = None) -> List[Tuple[int, float]]:
        """
        Processamento em streaming para dados hiperespectrais
        """
        results = []
        
        for tile in data_stream:
            try:
                class_id, confidence = self.process_tile(tile)
                results.append((class_id, confidence))
                
                if callback:
                    callback(class_id, confidence)
                    
            except Exception as e:
                print(f"Error processing tile: {e}")
                results.append((-1, 0.0))
        
        return results
    
    def get_performance_metrics(self) -> dict:
        """
        Coleta de metricas de performance da VPU
        """
        # Metricas disponiveis no OpenVINO
        return {
            "device": "MYRIAD",
            "model_precision": "FP16",
            "throughput": "Measured during inference",
            "latency": "Measured per inference",
            "power_consumption": "~1W nominal"
        }

# Exemplo de uso otimizado
def main():
    # Inicializacao da VPU
    vpu_processor = HyperspectralVPU("optimized_model.xml")
    
    # Simulacao de dados streaming
    def data_generator():
        # Simular tiles de 64x64 pixels com bandas selecionadas
        for i in range(100):
            yield np.random.rand(64, 64, 45)  # 45 bandas selecionadas
    
    # Callback para processamento em tempo real
    def result_callback(class_id, confidence):
        if confidence > 0.8:
            print(f"High confidence detection: Class {class_id}, Conf: {confidence:.3f}")
    
    # Processamento streaming
    results = vpu_processor.process_streaming(
        data_generator(), 
        callback=result_callback
    )
    
    # Analise de resultados
    print(f"Processed {len(results)} tiles")
    print(f"Average confidence: {np.mean([r[1] for r in results]):.3f}")

if __name__ == "__main__":
    main()
\end{lstlisting}

\subsection{Otimizacoes Especificas para VPU}

\subsubsection{Quantizacao e Optimizacao de Modelo}
Processo de otimizacao do modelo para a VPU:

\begin{lstlisting}[language=Python]
from openvino.tools import mo
from openvino.runtime import serialize
import openvino.runtime as ov

class VPUModelOptimizer:
    def __init__(self):
        self.core = ov.Core()
    
    def optimize_for_vpu(self, 
                        pytorch_model_path: str, 
                        output_path: str,
                        input_shape: tuple = (1, 20, 64, 64)):
        """
        Otimizacao de modelo PyTorch para VPU Myriad X
        """
        # Conversao para formato OpenVINO IR
        model = mo.convert_model(
            pytorch_model_path,
            input_shape=input_shape,
            compress_to_fp16=True,  # Importante para VPU
            mean_values=[0.485, 0.456, 0.406],  # Normalizacao padrao
            scale_values=[0.229, 0.224, 0.225]
        )
        
        # Configuracao especifica para Myriad X
        config = {
            "VPU_NUMBER_OF_SHAVES": 8,
            "VPU_NUMBER_OF_CMX_SLICES": 8,
            "VPU_TILING_CMX_LIMIT_KB": 200,
            "LOG_LEVEL": "LOG_INFO"
        }
        
        # Compilacao otimizada
        compiled_model = self.core.compile_model(
            model, 
            "MYRIAD", 
            config
        )
        
        # Serializacao do modelo otimizado
        serialize(model, output_path + ".xml", output_path + ".bin")
        
        return compiled_model
    
    def benchmark_model(self, model_path: str, iterations: int = 100):
        """
        Benchmark de performance na VPU
        """
        model = self.core.read_model(model_path)
        compiled_model = self.core.compile_model(model, "MYRIAD")
        
        # Preparar dados de teste
        input_shape = compiled_model.input().shape
        test_data = np.random.rand(*input_shape).astype(np.float32)
        
        # Warm-up
        for _ in range(10):
            request = compiled_model.create_infer_request()
            request.infer({compiled_model.input(): test_data})
        
        # Benchmark timing
        import time
        start_time = time.time()
        
        for _ in range(iterations):
            request = compiled_model.create_infer_request()
            request.infer({compiled_model.input(): test_data})
        
        end_time = time.time()
        
        avg_latency = (end_time - start_time) / iterations * 1000  # ms
        throughput = iterations / (end_time - start_time)  # fps
        
        return {
            "average_latency_ms": avg_latency,
            "throughput_fps": throughput,
            "total_time_s": end_time - start_time
        }
\end{lstlisting}

\section{Implementacao em GPU Embarcada}\label{sec:implementacao_gpu}

A implementacao para GPU embarcada utilizou CUDA otimizado para a arquitetura Maxwell do Jetson Nano.

\subsection{Kernels CUDA Otimizados}

\subsubsection{Kernel de Compressao Espectral}
Implementacao CUDA para selecao adaptativa de bandas:

\begin{lstlisting}[language=C++]
#include <cuda_runtime.h>
#include <cublas_v2.h>
#include <curand.h>

// Kernel para calculo de correlacao entre bandas
__global__ void compute_band_correlation(
    const float* hypercube,
    float* correlation_matrix,
    int height,
    int width,
    int bands,
    int band1_idx,
    int band2_idx
) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    int total_pixels = height * width;
    
    if (tid >= total_pixels) return;
    
    // Shared memory para reducao
    __shared__ float sum_x[256];
    __shared__ float sum_y[256];
    __shared__ float sum_xy[256];
    __shared__ float sum_x2[256];
    __shared__ float sum_y2[256];
    
    int local_tid = threadIdx.x;
    
    // Inicializar shared memory
    sum_x[local_tid] = 0.0f;
    sum_y[local_tid] = 0.0f;
    sum_xy[local_tid] = 0.0f;
    sum_x2[local_tid] = 0.0f;
    sum_y2[local_tid] = 0.0f;
    
    // Calcular indices para as bandas
    int band1_offset = band1_idx * total_pixels;
    int band2_offset = band2_idx * total_pixels;
    
    // Acumular valores para correlacao
    for (int i = tid; i < total_pixels; i += blockDim.x * gridDim.x) {
        float x = hypercube[band1_offset + i];
        float y = hypercube[band2_offset + i];
        
        sum_x[local_tid] += x;
        sum_y[local_tid] += y;
        sum_xy[local_tid] += x * y;
        sum_x2[local_tid] += x * x;
        sum_y2[local_tid] += y * y;
    }
    
    __syncthreads();
    
    // Reducao paralela
    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (local_tid < stride) {
            sum_x[local_tid] += sum_x[local_tid + stride];
            sum_y[local_tid] += sum_y[local_tid + stride];
            sum_xy[local_tid] += sum_xy[local_tid + stride];
            sum_x2[local_tid] += sum_x2[local_tid + stride];
            sum_y2[local_tid] += sum_y2[local_tid + stride];
        }
        __syncthreads();
    }
    
    // Thread 0 calcula correlacao final
    if (local_tid == 0) {
        float n = (float)total_pixels;
        float numerator = n * sum_xy[0] - sum_x[0] * sum_y[0];
        float denom_x = n * sum_x2[0] - sum_x[0] * sum_x[0];
        float denom_y = n * sum_y2[0] - sum_y[0] * sum_y[0];
        
        float correlation = numerator / sqrtf(denom_x * denom_y);
        
        // Armazenar resultado na matriz de correlacao
        correlation_matrix[band1_idx * bands + band2_idx] = correlation;
        correlation_matrix[band2_idx * bands + band1_idx] = correlation;
    }
}

// Kernel para selecao de bandas baseada em correlacao
__global__ void select_bands_kernel(
    const float* correlation_matrix,
    bool* selected_bands,
    int bands,
    float threshold
) {
    int band_idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (band_idx >= bands) return;
    
    // Primeira banda sempre selecionada
    if (band_idx == 0) {
        selected_bands[0] = true;
        return;
    }
    
    // Verificar correlacao com bandas ja selecionadas
    bool should_select = true;
    
    for (int i = 0; i < band_idx; i++) {
        if (selected_bands[i]) {
            float corr = fabsf(correlation_matrix[band_idx * bands + i]);
            if (corr >= threshold) {
                should_select = false;
                break;
            }
        }
    }
    
    selected_bands[band_idx] = should_select;
}

// Kernel de PCA incremental otimizado
__global__ void pca_transform_kernel(
    const float* input_data,
    const float* components,
    const float* mean_vector,
    float* output_data,
    int height,
    int width,
    int input_bands,
    int output_components
) {
    int pixel_idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total_pixels = height * width;
    
    if (pixel_idx >= total_pixels) return;
    
    // Shared memory para componentes principais (ate 32 componentes)
    __shared__ float shared_components[32 * 224];  // Max 224 input bands
    __shared__ float shared_mean[224];
    
    int tid = threadIdx.x;
    
    // Carregar mean vector para shared memory
    if (tid < input_bands) {
        shared_mean[tid] = mean_vector[tid];
    }
    
    // Carregar componentes principais para shared memory
    for (int i = tid; i < output_components * input_bands; i += blockDim.x) {
        if (i < output_components * input_bands) {
            shared_components[i] = components[i];
        }
    }
    
    __syncthreads();
    
    // Processar pixel atual
    for (int comp = 0; comp < output_components; comp++) {
        float result = 0.0f;
        
        for (int band = 0; band < input_bands; band++) {
            int input_offset = band * total_pixels + pixel_idx;
            float centered_value = input_data[input_offset] - shared_mean[band];
            result += centered_value * shared_components[comp * input_bands + band];
        }
        
        int output_offset = comp * total_pixels + pixel_idx;
        output_data[output_offset] = result;
    }
}

// Classe C++ para gerenciamento dos kernels
class CUDAHyperspectralProcessor {
private:
    float* d_hypercube;
    float* d_correlation_matrix;
    bool* d_selected_bands;
    float* d_pca_components;
    float* d_mean_vector;
    float* d_output_data;
    
    cublasHandle_t cublas_handle;
    cudaStream_t stream;
    
    int height, width, bands;
    int selected_band_count;
    int pca_components;
    
public:
    CUDAHyperspectralProcessor(int h, int w, int b, int pca_comp = 20) :
        height(h), width(w), bands(b), pca_components(pca_comp) {
        
        // Alocar memoria GPU
        size_t hypercube_size = height * width * bands * sizeof(float);
        size_t correlation_size = bands * bands * sizeof(float);
        size_t bands_size = bands * sizeof(bool);
        size_t components_size = pca_comp * bands * sizeof(float);
        size_t mean_size = bands * sizeof(float);
        size_t output_size = height * width * pca_comp * sizeof(float);
        
        cudaMalloc(&d_hypercube, hypercube_size);
        cudaMalloc(&d_correlation_matrix, correlation_size);
        cudaMalloc(&d_selected_bands, bands_size);
        cudaMalloc(&d_pca_components, components_size);
        cudaMalloc(&d_mean_vector, mean_size);
        cudaMalloc(&d_output_data, output_size);
        
        // Inicializar cuBLAS e stream
        cublasCreate(&cublas_handle);
        cudaStreamCreate(&stream);
    }
    
    ~CUDAHyperspectralProcessor() {
        cudaFree(d_hypercube);
        cudaFree(d_correlation_matrix);
        cudaFree(d_selected_bands);
        cudaFree(d_pca_components);
        cudaFree(d_mean_vector);
        cudaFree(d_output_data);
        
        cublasDestroy(cublas_handle);
        cudaStreamDestroy(stream);
    }
    
    void process_hyperspectral_data(
        const float* h_hypercube,
        float* h_output,
        float correlation_threshold = 0.95f
    ) {
        // Copiar dados para GPU
        cudaMemcpyAsync(
            d_hypercube, 
            h_hypercube, 
            height * width * bands * sizeof(float),
            cudaMemcpyHostToDevice,
            stream
        );
        
        // 1. Calcular matriz de correlacao
        compute_correlation_matrix(correlation_threshold);
        
        // 2. Aplicar PCA incremental
        apply_pca_transform();
        
        // 3. Copiar resultados de volta
        cudaMemcpyAsync(
            h_output,
            d_output_data,
            height * width * pca_components * sizeof(float),
            cudaMemcpyDeviceToHost,
            stream
        );
        
        cudaStreamSynchronize(stream);
    }
    
private:
    void compute_correlation_matrix(float threshold) {
        // Configuracao de grid e bloco
        int threads_per_block = 256;
        int total_pixels = height * width;
        int blocks = (total_pixels + threads_per_block - 1) / threads_per_block;
        
        // Calcular correlacoes para todas as combinacoes de bandas
        for (int i = 0; i < bands; i++) {
            for (int j = i + 1; j < bands; j++) {
                compute_band_correlation<<<blocks, threads_per_block, 0, stream>>>(
                    d_hypercube,
                    d_correlation_matrix,
                    height, width, bands,
                    i, j
                );
            }
        }
        
        // Selecionar bandas baseado na correlacao
        int band_blocks = (bands + threads_per_block - 1) / threads_per_block;
        select_bands_kernel<<<band_blocks, threads_per_block, 0, stream>>>(
            d_correlation_matrix,
            d_selected_bands,
            bands,
            threshold
        );
        
        cudaStreamSynchronize(stream);
    }
    
    void apply_pca_transform() {
        int threads_per_block = 256;
        int total_pixels = height * width;
        int blocks = (total_pixels + threads_per_block - 1) / threads_per_block;
        
        pca_transform_kernel<<<blocks, threads_per_block, 0, stream>>>(
            d_hypercube,
            d_pca_components,
            d_mean_vector,
            d_output_data,
            height, width,
            bands,
            pca_components
        );
        
        cudaStreamSynchronize(stream);
    }
};
\end{lstlisting}

\section{Desenvolvimento de Prototipos}\label{sec:prototipos}

Esta secao apresenta o desenvolvimento de prototipos funcionais que integram as implementacoes de hardware com interfaces praticas.

\subsection{Prototipo Agricola}

\subsubsection{Sistema Embarcado para Drone}
Integracao completa para aplicacao em agricultura de precisao:

\begin{lstlisting}[language=Python]
import asyncio
import numpy as np
from dataclasses import dataclass
from typing import Optional, Callable
import logging
import json
from datetime import datetime

@dataclass
class CropStressDetection:
    pixel_location: tuple
    stress_type: str  # 'nitrogen', 'phosphorus', 'potassium', 'water'
    severity: float  # 0.0 to 1.0
    confidence: float
    timestamp: datetime

class AgriculturalDroneSystem:
    def __init__(self, 
                 hardware_platform: str = "jetson_nano",
                 processing_mode: str = "real_time"):
        
        self.platform = hardware_platform
        self.mode = processing_mode
        
        # Inicializar processador baseado na plataforma
        if hardware_platform == "jetson_nano":
            from cuda_processor import CUDAHyperspectralProcessor
            self.processor = CUDAHyperspectralProcessor(64, 64, 224, 20)
        elif hardware_platform == "movidius":
            from vpu_processor import HyperspectralVPU
            self.processor = HyperspectralVPU("agricultural_model.xml")
        elif hardware_platform == "fpga":
            from fpga_interface import FPGAProcessor
            self.processor = FPGAProcessor()
        
        # Configuracao do sistema
        self.stress_thresholds = {
            'nitrogen': 0.75,
            'phosphorus': 0.70,
            'potassium': 0.65,
            'water': 0.80
        }
        
        # Sistema de logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # Callbacks para acoes em tempo real
        self.stress_callbacks = []
        
    def add_stress_callback(self, callback: Callable[[CropStressDetection], None]):
        """Adicionar callback para deteccoes de estresse"""
        self.stress_callbacks.append(callback)
    
    async def process_flight_mission(self, 
                                   flight_data_stream,
                                   gps_coordinates,
                                   field_metadata: dict):
        """
        Processamento de missao de voo completa
        """
        self.logger.info(f"Starting agricultural mission over {field_metadata['area_hectares']} hectares")
        
        stress_detections = []
        processed_tiles = 0
        
        async for tile_data, gps_coord in flight_data_stream:
            try:
                # Processamento do tile hiperespectral
                detections = await self._process_agricultural_tile(
                    tile_data, gps_coord, field_metadata
                )
                
                stress_detections.extend(detections)
                processed_tiles += 1
                
                # Executar callbacks para deteccoes criticas
                for detection in detections:
                    if detection.severity > self.stress_thresholds[detection.stress_type]:
                        for callback in self.stress_callbacks:
                            callback(detection)
                
                # Log de progresso
                if processed_tiles % 100 == 0:
                    self.logger.info(f"Processed {processed_tiles} tiles")
                    
            except Exception as e:
                self.logger.error(f"Error processing tile at {gps_coord}: {e}")
        
        # Relatorio final da missao
        mission_report = self._generate_mission_report(
            stress_detections, processed_tiles, field_metadata
        )
        
        return mission_report
    
    async def _process_agricultural_tile(self, 
                                       tile_data: np.ndarray,
                                       gps_coord: tuple,
                                       field_metadata: dict) -> list:
        """
        Processamento de um tile individual para deteccao de estresse
        """
        detections = []
        
        # Pre-processamento especifico para agricultura
        preprocessed = self._agricultural_preprocessing(tile_data, field_metadata)
        
        # Processamento via hardware embarcado
        if self.platform == "jetson_nano":
            results = await self._process_cuda(preprocessed)
        elif self.platform == "movidius":
            results = await self._process_vpu(preprocessed)
        elif self.platform == "fpga":
            results = await self._process_fpga(preprocessed)
        
        # Analise especifica para cada tipo de estresse
        for stress_type in ['nitrogen', 'phosphorus', 'potassium', 'water']:
            severity, confidence = self._analyze_stress_indicators(
                results, stress_type, field_metadata
            )
            
            if confidence > 0.7:  # Threshold minimo de confianca
                detection = CropStressDetection(
                    pixel_location=gps_coord,
                    stress_type=stress_type,
                    severity=severity,
                    confidence=confidence,
                    timestamp=datetime.now()
                )
                detections.append(detection)
        
        return detections
    
    def _agricultural_preprocessing(self, 
                                  data: np.ndarray, 
                                  field_metadata: dict) -> np.ndarray:
        """
        Pre-processamento especifico para aplicacoes agricolas
        """
        # Correcao atmosferica simplificada
        atmospheric_corrected = self._simple_atmospheric_correction(data)
        
        # Normalizacao baseada no tipo de cultura
        crop_type = field_metadata.get('crop_type', 'generic')
        normalized = self._crop_specific_normalization(atmospheric_corrected, crop_type)
        
        # Filtragem de ruido especifica para agricultura
        filtered = self._agricultural_noise_filter(normalized)
        
        return filtered
    
    def _simple_atmospheric_correction(self, data: np.ndarray) -> np.ndarray:
        """
        Correcao atmosferica simplificada para processamento embarcado
        """
        # Metodo de Dark Object Subtraction (DOS) simplificado
        dark_object_values = np.percentile(data, 1, axis=(0, 1))
        corrected = data - dark_object_values[np.newaxis, np.newaxis, :]
        
        # Clipping para evitar valores negativos
        return np.clip(corrected, 0, None)
    
    def _crop_specific_normalization(self, 
                                   data: np.ndarray, 
                                   crop_type: str) -> np.ndarray:
        """
        Normalizacao especifica por tipo de cultura
        """
        # Parametros de normalizacao por cultura
        normalization_params = {
            'soy': {'mean': 0.3, 'std': 0.15},
            'corn': {'mean': 0.35, 'std': 0.18},
            'wheat': {'mean': 0.28, 'std': 0.12},
            'generic': {'mean': 0.3, 'std': 0.15}
        }
        
        params = normalization_params.get(crop_type, normalization_params['generic'])
        
        # Aplicar normalizacao
        mean_data = np.mean(data, axis=(0, 1), keepdims=True)
        std_data = np.std(data, axis=(0, 1), keepdims=True)
        
        normalized = (data - mean_data) / (std_data + 1e-8)  # Evitar divisao por zero
        
        # Escalar para range especifico da cultura
        scaled = normalized * params['std'] + params['mean']
        
        return scaled
    
    def _agricultural_noise_filter(self, data: np.ndarray) -> np.ndarray:
        """
        Filtro de ruido especifico para agricultura
        """
        # Filtro mediano para reduzir ruido impulsivo
        from scipy.ndimage import median_filter
        
        filtered = np.copy(data)
        
        # Aplicar filtro mediano apenas em bandas especificas sensiveis ao ruido
        noise_sensitive_bands = [10, 20, 30, 40, 50]  # Bandas tipicas com mais ruido
        
        for band in noise_sensitive_bands:
            if band < data.shape[2]:
                filtered[:, :, band] = median_filter(data[:, :, band], size=3)
        
        return filtered
    
    def _analyze_stress_indicators(self, 
                                 processed_data: dict,
                                 stress_type: str,
                                 field_metadata: dict) -> tuple:
        """
        Analise especifica para diferentes tipos de estresse
        """
        # Indices espectrais especificos para cada tipo de estresse
        stress_indices = {
            'nitrogen': self._calculate_nitrogen_indices(processed_data),
            'phosphorus': self._calculate_phosphorus_indices(processed_data),
            'potassium': self._calculate_potassium_indices(processed_data),
            'water': self._calculate_water_stress_indices(processed_data)
        }
        
        indices = stress_indices[stress_type]
        
        # Classificacao baseada em thresholds especificos
        severity = self._classify_stress_severity(indices, stress_type, field_metadata)
        confidence = self._calculate_confidence(indices, stress_type)
        
        return severity, confidence
    
    def _calculate_nitrogen_indices(self, data: dict) -> dict:
        """
        Calcular indices espectrais relacionados ao nitrogenio
        """
        # Assumindo que os dados processados contem reflectancias especificas
        red_edge = data.get('red_edge_reflectance', 0.5)
        nir = data.get('nir_reflectance', 0.6)
        red = data.get('red_reflectance', 0.3)
        
        # NDVI modificado para nitrogenio
        ndvi = (nir - red) / (nir + red + 1e-8)
        
        # Red Edge Position
        rep = red_edge / nir if nir > 0 else 0
        
        # Chlorophyll Index
        ci = (nir / red_edge) - 1 if red_edge > 0 else 0
        
        return {
            'ndvi': ndvi,
            'red_edge_position': rep,
            'chlorophyll_index': ci
        }
    
    def _generate_mission_report(self, 
                               detections: list,
                               total_tiles: int,
                               field_metadata: dict) -> dict:
        """
        Gerar relatorio completo da missao
        """
        # Agrupar deteccoes por tipo de estresse
        stress_summary = {}
        for detection in detections:
            stress_type = detection.stress_type
            if stress_type not in stress_summary:
                stress_summary[stress_type] = {
                    'count': 0,
                    'avg_severity': 0,
                    'max_severity': 0,
                    'locations': []
                }
            
            stress_summary[stress_type]['count'] += 1
            stress_summary[stress_type]['avg_severity'] += detection.severity
            stress_summary[stress_type]['max_severity'] = max(
                stress_summary[stress_type]['max_severity'], 
                detection.severity
            )
            stress_summary[stress_type]['locations'].append(detection.pixel_location)
        
        # Calcular medias
        for stress_type in stress_summary:
            if stress_summary[stress_type]['count'] > 0:
                stress_summary[stress_type]['avg_severity'] /= stress_summary[stress_type]['count']
        
        # Relatorio final
        report = {
            'mission_metadata': {
                'field_area_hectares': field_metadata['area_hectares'],
                'crop_type': field_metadata.get('crop_type', 'unknown'),
                'processing_platform': self.platform,
                'total_tiles_processed': total_tiles,
                'total_detections': len(detections),
                'mission_timestamp': datetime.now().isoformat()
            },
            'stress_analysis': stress_summary,
            'recommendations': self._generate_recommendations(stress_summary),
            'performance_metrics': self._get_performance_metrics()
        }
        
        return report
    
    def _generate_recommendations(self, stress_summary: dict) -> list:
        """
        Gerar recomendacoes baseadas nas deteccoes
        """
        recommendations = []
        
        for stress_type, data in stress_summary.items():
            if data['count'] > 0:
                if data['avg_severity'] > 0.8:
                    urgency = "ALTA"
                elif data['avg_severity'] > 0.6:
                    urgency = "MEDIA"
                else:
                    urgency = "BAIXA"
                
                recommendation = {
                    'stress_type': stress_type,
                    'urgency': urgency,
                    'affected_area_percent': (data['count'] / 100) * 100,  # Aproximacao
                    'action_required': self._get_action_for_stress(stress_type, data['avg_severity'])
                }
                
                recommendations.append(recommendation)
        
        return recommendations
    
    def _get_action_for_stress(self, stress_type: str, severity: float) -> str:
        """
        Determinar acao recomendada para cada tipo de estresse
        """
        actions = {
            'nitrogen': {
                0.8: "Aplicacao imediata de fertilizante nitrogenado",
                0.6: "Agendar aplicacao de nitrogenio na proxima semana",
                0.0: "Monitorar desenvolvimento"
            },
            'phosphorus': {
                0.8: "Aplicacao de fertilizante fosfatado",
                0.6: "Considerar adubacao com fosforo",
                0.0: "Continuar monitoramento"
            },
            'water': {
                0.8: "Irrigacao imediata necessaria",
                0.6: "Programar irrigacao em 24-48h",
                0.0: "Manter cronograma de irrigacao atual"
            }
        }
        
        if stress_type in actions:
            for threshold in sorted(actions[stress_type].keys(), reverse=True):
                if severity >= threshold:
                    return actions[stress_type][threshold]
        
        return "Continuar monitoramento regular"

# Exemplo de uso do sistema
async def main():
    # Inicializar sistema
    drone_system = AgriculturalDroneSystem(
        hardware_platform="jetson_nano",
        processing_mode="real_time"
    )
    
    # Configurar callback para alertas criticos
    def critical_stress_alert(detection: CropStressDetection):
        print(f"ALERTA CRITICO: {detection.stress_type} detectado com severidade {detection.severity:.2f}")
        # Aqui poderia enviar SMS, email, ou comandar aplicacao automatica
    
    drone_system.add_stress_callback(critical_stress_alert)
    
    # Metadados do campo
    field_info = {
        'area_hectares': 50,
        'crop_type': 'soy',
        'planting_date': '2025-03-15',
        'irrigation_system': 'pivot'
    }
    
    # Simular stream de dados de voo
    async def flight_data_stream():
        for i in range(1000):  # Simular 1000 tiles
            # Simular dados hiperespectrais (64x64 pixels, 224 bandas)
            tile_data = np.random.rand(64, 64, 224) * 0.8 + 0.1
            gps_coord = (-22.8579, -47.0659)  # Coordenadas exemplo
            
            yield tile_data, gps_coord
            await asyncio.sleep(0.1)  # Simular tempo de aquisicao
    
    # Executar missao
    report = await drone_system.process_flight_mission(
        flight_data_stream(),
        [(-22.8579, -47.0659)],  # Coordenadas do campo
        field_info
    )
    
    # Imprimir relatorio
    print(json.dumps(report, indent=2, default=str))

if __name__ == "__main__":
    asyncio.run(main())
\end{lstlisting}

Este capitulo demonstra a implementacao pratica das estrategias de otimizacao em diferentes plataformas de hardware embarcado, desde simulacoes detalhadas via GHDL ate prototipos funcionais completos. As implementacoes validam a viabilidade tecnica das estrategias propostas e estabelecem a base para os resultados apresentados no proximo capitulo. 